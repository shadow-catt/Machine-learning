{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4023 Machine Learning : Ensemble Learning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise, you'll explore different ensemble methods and how does ensemble improves the performance of a machine learning model. There are three parts in this exercise:\n",
    "1. Simple ensemble strategy: majority voting\n",
    "2. Bagging Method\n",
    "3. Boosting Method: Adaboost\n",
    "\n",
    "The dataset we use for this exercise is a cancer dataset with 699 instances and a total number of 9 features labeled in either benign or malignant classes (0 for benign, 1 for malignant). The dataset only contains numeric values and has been normalized.\n",
    "\n",
    "Many methods will use random generator, e.g., train-test split, decision tree model, bagging boostramp sample generation, therefore, we can set the seed to a fixed number in order to achieve same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.379749</td>\n",
       "      <td>0.237164</td>\n",
       "      <td>0.245271</td>\n",
       "      <td>0.200763</td>\n",
       "      <td>0.246225</td>\n",
       "      <td>0.346352</td>\n",
       "      <td>0.270863</td>\n",
       "      <td>0.207439</td>\n",
       "      <td>0.065490</td>\n",
       "      <td>0.344778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.312860</td>\n",
       "      <td>0.339051</td>\n",
       "      <td>0.330213</td>\n",
       "      <td>0.317264</td>\n",
       "      <td>0.246033</td>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.270929</td>\n",
       "      <td>0.339293</td>\n",
       "      <td>0.190564</td>\n",
       "      <td>0.475636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "count       699.000000               699.000000                699.000000   \n",
       "mean          0.379749                 0.237164                  0.245271   \n",
       "std           0.312860                 0.339051                  0.330213   \n",
       "min           0.000000                 0.000000                  0.000000   \n",
       "25%           0.111111                 0.000000                  0.000000   \n",
       "50%           0.333333                 0.000000                  0.000000   \n",
       "75%           0.555556                 0.444444                  0.444444   \n",
       "max           1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei  \\\n",
       "count         699.000000                   699.000000   699.000000   \n",
       "mean            0.200763                     0.246225     0.346352   \n",
       "std             0.317264                     0.246033     0.364071   \n",
       "min             0.000000                     0.000000     0.000000   \n",
       "25%             0.000000                     0.111111     0.100000   \n",
       "50%             0.000000                     0.111111     0.100000   \n",
       "75%             0.333333                     0.333333     0.500000   \n",
       "max             1.000000                     1.000000     1.000000   \n",
       "\n",
       "       Bland Chromatin  Normal Nucleoli     Mitoses       Class  \n",
       "count       699.000000       699.000000  699.000000  699.000000  \n",
       "mean          0.270863         0.207439    0.065490    0.344778  \n",
       "std           0.270929         0.339293    0.190564    0.475636  \n",
       "min           0.000000         0.000000    0.000000    0.000000  \n",
       "25%           0.111111         0.000000    0.000000    0.000000  \n",
       "50%           0.222222         0.000000    0.000000    0.000000  \n",
       "75%           0.444444         0.333333    0.000000    1.000000  \n",
       "max           1.000000         1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "data = pd.read_csv(\"cancer_normalized.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Simple Ensemble Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at a simple ensemble technique for classification: majority voting. In this method, multiple models are used to make predictions for each data instance. The predictions by each model are considered as a **vote**. The prediction which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "Scikit-Learn provides us with some handy functions that we can use to accomplish this.\n",
    "- The ``VotingClassifier`` takes in a list of different estimators as arguments and a voting method. The ``hard`` voting method uses the predicted labels and a majority rules system, while the ``soft`` voting method predicts a label based on the sum of the predicted probabilities.\n",
    "\n",
    "Here, we use three models, *Decision Tree*, *SVM* and *LogisticRegression*, for voting and adopt 10-fold cross validation. Report the mean accuracy of **individual classifiers and the ensemble by applying the majority voting strategy (**hard voting**).** Compare the performance. \n",
    "- Note: For DecisionTreeClassifier() implementation, the features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "seed = 7\n",
    "\n",
    "# your implementation here\n",
    "# declare the result \n",
    "\n",
    "# for individual classifiers\n",
    "decision_tree_cls = DecisionTreeClassifier(random_state=seed)\n",
    "logistic_reg_cls = LogisticRegression(random_state=seed)\n",
    "svc_cls = SVC(random_state=seed)\n",
    "# for voting class\n",
    "voting_cls = VotingClassifier(estimators=[(\"log_clf\", LogisticRegression(random_state=seed)),\n",
    "                                         (\"svm_clf\", SVC(random_state= seed)),\n",
    "                                         (\"Dec_clf\", DecisionTreeClassifier(random_state=seed))])\n",
    "# binary classification problem\n",
    "# define the input x and label y\n",
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "# mean score of voting \n",
    "mean_scores_voting = cross_val_score(voting_cls, x,y,cv = 10).mean()\n",
    "# mean score of decision tree\n",
    "mean_score_des = cross_val_score(decision_tree_cls, x, y, cv =10).mean()\n",
    "# mean score of SVC\n",
    "mean_score_svc = cross_val_score(svc_cls, x, y, cv = 10).mean()\n",
    "# mean score of Logistic regression\n",
    "mean_score_lr = cross_val_score(logistic_reg_cls, x, y, cv = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9485093167701864,\n",
       " 0.9628571428571429,\n",
       " 0.9685507246376812,\n",
       " 0.9642650103519671)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_score_des, mean_score_lr, mean_score_svc, mean_scores_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the bagging method by using decision tree as the base learning algorithm. Scikit-Learn provides us a module of ``BaggingClassifier``, we can provide the base learning model and the number of estimation models. Try to set the number of estimators to 100 and report the mean accuracy of the ensemble using 10-fold cross validation. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "# your implemenation here\n",
    "# initialize bagging method with single decision tree\n",
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=seed),\n",
    "                       n_estimators=num_trees, random_state= seed)\n",
    "bagging_score = cross_val_score(clf, x, y, cv = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9557142857142857, 0.9485093167701864)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_score, mean_score_des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialized a 10-fold cross-validation fold. After that, we instantiated a Decision Tree Classifier with 100 trees and wrapped it in a Bagging-based Ensemble. The accuracy improved to 95.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also provides access to the ``RandomForestClassifier``, which is a modification of the decision tree classification. Use random forest model and report the mean accuracy by using 10-folds cross-validation. Number of trees set to 100.\n",
    "\n",
    "**Compare the performance of RandomForestClassifier with bagged decision tree and give the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# your implementation here...\n",
    "# implement the Random Forest\n",
    "# also set default to true\n",
    "rdf_cls = RandomForestClassifier(random_state=seed)\n",
    "mean_score_rdf = cross_val_score(rdf_cls, x, y, cv = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9671428571428571, 0.9557142857142857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_score_rdf, bagging_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison and analysis**:\n",
    "\n",
    "## Analysis\n",
    "Bagging here is known as **bootstrap aggregating**. This method use to train a number of base learners from a different bootstrap sample by calling decision tree basic method here. The final combine method is the voting, which is hard voting here. Then, for Random Forest, still use boostrap here for sampling, which make sure each tree has same training data and have some difference, which means the learning ability will not be reduced too much even for small dataset. Also, random forest randomly select features for all candidate features, and finally get the best feature. So, random forest have:\n",
    "1. better training effiency \n",
    "2. more robustness since random forest focus on different candidate features while Bagging only have sampling robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you use AdaBoost classification by boosting the ``decision stump``(**one-level decision tree**).Try to set the number of rounds to 100 and report the performance of the ensemble. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# your implementation here...\n",
    "adaboost_cls = AdaBoostClassifier(n_estimators=100, random_state=seed,\n",
    "                                 base_estimator = DecisionTreeClassifier(max_depth=1))\n",
    "adaboost_score = cross_val_score(adaboost_cls, x,y, cv = 10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9485093167701864, 0.9585507246376814)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_score_des, adaboost_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
